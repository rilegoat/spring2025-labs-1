{
  "model": "llama3.2",
  "options": {
    "temperature": 0,
    "max_tokens": 100
  },
  "messages": [
      { "role": "system", "content": ""}
  ]
}