# Prompt Engineering Process

### Step 1
#### Intention
>What is the improvement that you intend to make?

Today, my intention was to initialize my LLM's code so that it would begin acting like a Dungeon Master. To do this, I set up a simple call-and-response between the user and the system. The system is initially instructed to act as a Dungeon Master, specializing in single-player oriented games. Initially I chose llama3.2 as my model because I wanted to test the capabilities of it before finding a less RAM-intensive model.

#### Action/Change
>Why do you think this action/change will improve the agent?

My initial settings feel like they will add enough constraints to shape the model into giving the responses I'm looking for while still allowing room for creativity. My temperature is initially set to 0.7, but I will experiment with making this value larger if I desire more creativity from the model. By having the model produce responses before the user, I'm able to get a user's input to gracefully exit the game, which also allows me to track my progress towards improving my model.

#### Result
>What was the result?

The result was promising, and the model did a good job at replicating the style of a Dungeon Master. It succinctly explained the premise and rules of Dungeons & Dragons, and it also fit itself into a personable and emotive expression which makes the user feel as if they're interacting with a real person.
On the other hand, responses take a very long time to generate---typically waiting close to a minute for responses, which would not be practical in a D&D setting. To attempt to solve this, I'm going to look into models that use less RAM and I'll be testing those models to see which of them seem to be better with narrative storytelling.

#### Reflection/Analysis of the result. 
>Why do you think it did or did not work?

I think this worked because of the constraints I added in the initial message---by telling the model to "emulate the prose and format of a Dungeons & Dragons Dungeon Master," it strongly filters and shapes the word choice and text formatting of my responses. This results in a much more authentic experience than if responses were formatted as simple paragraphs with less rigid structure.

### Step 2
#### Intention
>What is the improvement that you intend to make?

Today, I wanted to improve the relevancy and concision of my model's responses. I noticed that in my previous iteration, the AI stepped into third person and also began reciting slightly irrelevant information. I wanted to improve this with this iteration.

#### Action/Change
>Why do you think this action/change will improve the agent?

I adjusted the temperature and added a max_tokens tag that should both help stabilize and improve the response generated by the model. I tried having a temperature of 0.7 initially, but it seemed like responses were a little too erratic,

#### Result
>What was the result?

The response I got felt very rational and reasonable, and also felt more natural. The summary it gave me after I asked "What is Dungeons & Dragons?" felt natural and didn't feel contrived, and I like that the model sounded more personable and was less verbose than previous responses.

#### Reflection/Analysis of the result. 
>Why do you think it did or did not work?

By decreasing the temperature slightly, I think the model opted for some words and a direction that would be more appealing and natural to the user. The max_tokens tag also felt like it helped in terms of keeping the response short and sweet, rather than several paragraphs long. I'm happy with this improvement, and next I plan to try different models to see if the language choice in other models feels more fitting for a Dungeons & Dragons DM.